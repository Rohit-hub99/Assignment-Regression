{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Questions and Answers"
      ],
      "metadata": {
        "id": "0ZFCn5REkoey"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. What is Simple Linear Regression?\n",
        "  - Simple Linear Regression is a basic and widely used statistical method that models the relationship between two variables:\n",
        "    - One independent variable (X) — the input or predictor\n",
        "    - One dependent variable (Y) — the output or response\n",
        "\n",
        "  - Definition:\n",
        "   - Simple Linear Regression fits a straight line to the data that best represents the relationship between X and Y.\n",
        "\n",
        "\n",
        "\n",
        "# 2. What are the key assumptions of Simple Linear Regression ?\n",
        "  - Key Assumptions:\n",
        "\n",
        "   - Linearity\n",
        "     - The relationship between the independent variable (X) and the dependent variable (Y) should be linear.\n",
        "     - You can check this by plotting a scatter plot and seeing if the data points align roughly along a straight line.\n",
        "\n",
        "   - Independence of Errors\n",
        "     - The residuals (differences between actual and predicted Y values) should be independent.\n",
        "     - This means the error for one observation should not influence the error of another.\n",
        "     - Especially important in time series data — violations can lead to autocorrelation.\n",
        "\n",
        "   - Homoscedasticity\n",
        "     - The residuals should have constant variance across all values of X.\n",
        "     - If residuals spread out more as X increases (or decreases), it's called heteroscedasticity, which violates this assumption.\n",
        "     - You can check this using a residuals vs fitted values plot — look for a random scatter, not a funnel shape.\n",
        "\n",
        "   - Normality of Residuals\n",
        "     - The residuals (errors) should be normally distributed, especially important for inference like confidence intervals or hypothesis testing.\n",
        "     - Can be checked using:\n",
        "       - Histogram of residuals\n",
        "       - Q-Q plot (quantile-quantile plot)\n",
        "\n",
        "   - No Perfect Multicollinearity (only in Multiple Linear Regression)\n",
        "     - Not required in Simple Linear Regression (only one independent variable), but worth noting if you're planning to move to multiple regression later.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 3. What does the coefficient m represent in the equation Y=mX+c ?\n",
        "  - The coefficient m represents the slope of the line — and it's super important in understanding how X and Y relate.\n",
        "\n",
        "  - It tells you how much Y changes for a one-unit increase in X.\n",
        "\n",
        "\n",
        "\n",
        "# 4. What does the intercept c represent in the equation Y=mX+c ?\n",
        "  - The coefficient c represents the intercept — also known as the Y-intercept.\n",
        "  - It is the value of Y when X = 0.\n",
        "  - In other words, it's the point where the line crosses the Y-axis on a graph.\n",
        "\n",
        "\n",
        "\n",
        "# 5. How do we calculate the slope m in Simple Linear Regression ?\n",
        "  - Simple Linear Regression Line Equation:\n",
        "     \n",
        "     Y = mX + c\n",
        "\n",
        " - Where:\n",
        "   - Y: Predicted value (dependent variable)\n",
        "   - X: Input value (independent variable)\n",
        "   - m: Slope of the line\n",
        "   - c: Y-intercept (value of Y when X = 0)\n",
        "\n",
        " - Slope (m) — Using Means:\n",
        "    - The slope m is calculated by dividing the sum of the product of the difference between each X value and the mean of X, and the difference between each Y value and the mean of Y, by the sum of the squared differences between each X value and the mean of X.\n",
        "\n",
        " - Slope (m) — Using Summation:\n",
        "   - The slope m is calculated as:\n",
        "   - the number of observations multiplied by the sum of the product of X and Y, minus the product of the sum of X and the sum of Y, all divided by the number of observations multiplied by the sum of squares of X,minus the square of the sum of X.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 6. What is the purpose of the least squares method in Simple Linear Regression ?\n",
        "  - The least squares method is used to find the best-fitting line through the data points by minimizing the sum of the squared differences between the actual values and the predicted values.\n",
        "\n",
        "  - For each data point, we calculate the error (also called the residual) between:\n",
        "  - These errors are squared (to make them all positive and emphasize larger errors)\n",
        "  - Then, we add up all the squared errors\n",
        "  - The line with the smallest total squared error is chosen as the best-fit line\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression ?\n",
        "  - R² (R-squared) measures how well the regression line explains the variability in the dependent variable (Y) based on the independent variable (X).\n",
        "\n",
        "  - It tells you how much of the variation in Y is explained by X.\n",
        "  - The value of R² ranges between 0 and 1.\n",
        "\n",
        "  - Interpretation:\n",
        "    \n",
        "    - R² Value                Interpretation\n",
        "    - 0                       The model explains none of the variability in Y\n",
        "    - 0.25\t                  The model explains 25% of the variability in Y\n",
        "    - 0.75                    The model explains 75% of the variability in Y\n",
        "    - 1                       The model explains 100% of the variability — perfect fit\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 8. What is Multiple Linear Regression ?\n",
        "  - Multiple Linear Regression (MLR) is an extension of Simple Linear Regression where we use two or more independent variables (X₁, X₂, ..., Xₙ) to predict a single dependent variable (Y).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 9. What is the main difference between Simple and Multiple Linear Regression ?\n",
        "  - Simple Linear Regression:\n",
        "    - Number of independent variables :- One (only one predictor variable)\n",
        "    - Equation format :- Y=mX+c\n",
        "    - Purpose :- Understand the relationship between X and Y\n",
        "    - Example :- Predict marks based on hours studied\n",
        "\n",
        "  - Multiple Linear Regression:\n",
        "    - Number of independent variables :- Two or more predictor variables\n",
        "    - Equation format :-\n",
        "    - Purpose :- Understand how multiple Xs together affect Y\n",
        "    - Example :- Predict house price based on size, bedrooms, location\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 10.  What are the key assumptions of Multiple Linear Regression ?\n",
        "  - Key Assumptions of Multiple Linear Regression:\n",
        "    \n",
        "    - 1. Linearity:\n",
        "      - The relationship between the dependent variable and each independent variable is linear.\n",
        "      - Example: If you double an input (X), Y should increase (or decrease) proportionally.\n",
        "\n",
        "    - 2. Independence of Errors (No Autocorrelation):\n",
        "      - Especially important in time series data\n",
        "      - Can be tested using the Durbin-Watson statistic\n",
        "\n",
        "    - 3. Homoscedasticity\n",
        "      - In other words, the spread of errors should be equal across the regression line.\n",
        "      - You can test this using residual plots\n",
        "\n",
        "    - 4. No Multicollinearity\n",
        "      - High correlation among predictors can distort the influence of each variable.\n",
        "      - Use VIF (Variance Inflation Factor) to check multicollinearity.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model ?\n",
        "  - Heteroscedasticity occurs when the variance of the residuals (errors) is not constant across all levels of the independent variables.\n",
        "\n",
        "  - In contrast, homoscedasticity (which is desired) means the spread of the residuals stays fairly even throughout.\n",
        "\n",
        "  - Why is it a problem?\n",
        "    - Heteroscedasticity doesn’t affect the model's ability to predict, but it does affect the reliability of the model’s statistical inferences, such as:\n",
        "      - Standard errors of coefficients\n",
        "      - t-tests and p-values\n",
        "      - Confidence intervals\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 12.  How can you improve a Multiple Linear Regression model with high multicollinearity ?\n",
        "  - Multicollinearity happens when two or more independent variables in your regression model are highly correlated with each other.\n",
        "\n",
        "  - How to Improve the Model:\n",
        "    - 1. Remove Highly Correlated Predictors\n",
        "      - If two variables are providing the same information, drop one of them.\n",
        "  \n",
        "  - Combine Variables\n",
        "    - Use domain knowledge to create a new feature by combining related ones (e.g., total income = salary + bonus)\n",
        "\n",
        "  - Apply Dimensionality Reduction\n",
        "    - Use Principal Component Analysis (PCA) to create uncorrelated components\n",
        "    - This helps especially when you have many variables\n",
        "\n",
        "  - Use Regularization Techniques\n",
        "    - Switch to models that handle multicollinearity better:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 13. What are some common techniques for transforming categorical variables for use in regression models ?\n",
        "  - 1. One-Hot Encoding (Dummy Variables)\n",
        "   - Creates a new binary column for each category (0 or 1).\n",
        "   - The variable is nominal (no natural order), e.g., City, Color, Genre.\n",
        "\n",
        "  - 2. Label Encoding\n",
        "   - Assigns each category a unique number (e.g., 0, 1, 2...)\n",
        "   - The variable is ordinal (has a natural order), like Education = [High School, Graduate, Postgraduate].\n",
        "\n",
        "  - 3. Ordinal Encoding\n",
        "   - Similar to label encoding but manually assigns values based on meaningful order.\n",
        "   - You want control over the order or weight (e.g., rating = Poor, Average, Good, Excellent)\n",
        "\n",
        "  - 4. Binary Encoding\n",
        "   - First converts categories to binary, then splits the binary digits into separate columns. It’s a middle ground between one-hot and label encoding.\n",
        "   - You have many categories (like 100+ unique values)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 14. What is the role of interaction terms in Multiple Linear Regression ?\n",
        "  - Interaction terms capture the combined effect of two (or more) independent variables on the dependent variable — when the effect of one variable depends on the value of another.\n",
        "\n",
        "  Let’s say we are predicting sales using:\n",
        "    - TV_Ads\n",
        "    - Online_Ads\n",
        "\n",
        "    - Individually, both increase sales. But when combined, they might have a greater-than-expected impact — like they boost each other’s effects.\n",
        "    - This synergy is what an interaction term captures.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression ?\n",
        "  - Key Differences:\n",
        "    \n",
        "    - Simple Linear Regression\n",
        "     - of variables:- One independent variable\n",
        "     - Intercept means:- Y when X = 0\n",
        "     - Interpretability:- Usually meaningful\n",
        "\n",
        "    - Multiple Linear Regression\n",
        "     - of variables:- Two or more independent variables\n",
        "     - Intercept means:- Y when all X₁, X₂, ..., Xₙ = 0\n",
        "     - Interpretability:- Sometimes not meaningful (depends on context)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 16.  What is the significance of the slope in regression analysis, and how does it affect predictions ?\n",
        "  - The slope tells us how much the dependent variable (Y) changes for a one-unit increase in an independent variable (X) — keeping other variables constant (in MLR).\n",
        "\n",
        "  - How the Slope Affects Predictions:\n",
        "   - Magnitude\n",
        "     - A large slope → big change in Y when X changes\n",
        "     - A small slope → Y is less sensitive to X\n",
        "    \n",
        "   - Sign (Positive/Negative)\n",
        "     - Positive slope → as X increases, Y increases\n",
        "     - Negative slope → as X increases, Y decreases\n",
        "\n",
        "   - Zero Slope\n",
        "     - Means no linear relationship between X and Y\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 17. How does the intercept in a regression model provide context for the relationship between variables ?\n",
        "  - The intercept is the predicted value of the dependent variable (Y) when all independent variables (X’s) are 0.\n",
        "\n",
        "  - How It Provides Context:\n",
        "    - 1. Acts as a Starting Point\n",
        "      - It gives the baseline level of Y — i.e., where your predictions begin when no influence from predictors exists.\n",
        "      - Like the \"default\" condition.\n",
        "\n",
        "    - 2. Helps Anchor the Line/Plane\n",
        "      - The intercept determines where the regression line cuts the Y-axis.\n",
        "      - In multiple regression, it anchors the model in multi-dimensional space.\n",
        "\n",
        "    - 3. Makes the Slope Meaningful\n",
        "      - Without the intercept, the slope’s impact would float without reference.\n",
        "      - It helps in translating how much change the slopes create from the baseline.\n",
        "  \n",
        "    - 4. Gives Insight into Edge Cases\n",
        "      - If an intercept is negative or unreasonably high, it might flag:\n",
        "        - Data issues\n",
        "        - Model misspecification\n",
        "        - That zero values for predictors aren’t realistic (e.g., 0 kg weight, 0 years old)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 18. What are the limitations of using R² as a sole measure of model performance ?\n",
        "  - It tells us the proportion of the variance in the dependent variable that is explained by the independent variables.\n",
        "  - Ranges from 0 to 1 (or 0% to 100%)\n",
        "\n",
        "  - Limitations of Using R² Alone:\n",
        "    - 1. R² Always Increases When You Add Variables\n",
        "      - Even if the new variable is irrelevant, R² will still increase.\n",
        "      - This can lead to overfitting, especially in multiple linear regression.\n",
        "      - Use Adjusted R² instead — it penalizes unnecessary variables.\n",
        "\n",
        "    - 2. Doesn’t Indicate Predictive Power on New Data\n",
        "      - A model may have a high R² on training data but perform poorly on test data.\n",
        "      - This means it’s fitting noise (overfitting), not capturing general patterns.\n",
        "      - Use cross-validation or test RMSE/MAE for generalization checks.\n",
        "    \n",
        "    - 3. R² Can Be Misleading with Non-Linear Relationships\n",
        "      - R² assumes a linear relationship.\n",
        "      - In non-linear models, a low R² doesn’t mean the model is bad — it just means the variance isn't explained linearly.\n",
        "      - Plot residuals or consider non-linear models if needed.\n",
        "    \n",
        "    - 4. Doesn’t Tell You Whether the Coefficients Are Significant\n",
        "      - You might have a good R², but some predictors may still be statistically insignificant.\n",
        "      - Always check p-values and confidence intervals for each variable.\n",
        "    \n",
        "    - 5. Not Comparable Across Different Response Variables\n",
        "      - You can’t compare R² values between models predicting different dependent variables (e.g., sales vs. temperature).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 19.  How would you interpret a large standard error for a regression coefficient ?\n",
        "  - In regression, each coefficient (slope) comes with a standard error (SE), which measures:\n",
        "    -  The variability of the estimated coefficient across different samples of data.\n",
        "    - It tells us how precise or uncertain that estimate is.\n",
        "\n",
        "  - A large standard error means the coefficient estimate is not very precise.\n",
        "\n",
        "  - The coefficient might not be significantly different from 0\n",
        "    - You can’t confidently say the predictor has an effect on the outcome.\n",
        "  \n",
        "  - It leads to a wide confidence interval\n",
        "    - Example:\n",
        "       - Coefficient=3.5, Standard Error=2.8⇒CI is wide: (–2.1, 9.1)\n",
        "      \n",
        "    - That’s a lot of uncertainty about the true value!\n",
        "\n",
        "  - Low statistical significance (High p-value)\n",
        "    - High SE leads to a small t-statistic, which increases the p-value → meaning the coefficient may not be meaningful.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 20.  How can heteroscedasticity be identified in residual plots, and why is it important to address it ?\n",
        "  - Heteroscedasticity means that the variance of the residuals (errors) is not constant across all levels of the independent variable(s).\n",
        "\n",
        "  - How to Identify Heteroscedasticity in Residual Plots\n",
        "    - A residual plot is a scatter plot of:\n",
        "      - X-axis: Predicted values (or independent variable)\n",
        "      - Y-axis: Residuals (actual – predicted values)\n",
        "\n",
        "    - If the residuals are:\n",
        "      - Randomly scattered around 0\n",
        "      - With a constant spread (equal “noise” throughout)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R² ?\n",
        "  - R² (Coefficient of Determination):\n",
        "    - Shows how much of the variation in the target variable is explained by the model.\n",
        "  - Always increases when you add more predictors — even irrelevant ones.\n",
        "\n",
        "  - Adjusted R²:\n",
        "    - Adjusts R² based on the number of predictors and sample size\n",
        "    - Increases only if a new variable genuinely improves the model.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 22. Why is it important to scale variables in Multiple Linear Regression ?\n",
        "  - Helps with Interpretability of Coefficients\n",
        "    - If one variable is in kilometers (range: 0–1000) and another is in percentages (0–1), their coefficients are not directly comparable.\n",
        "\n",
        "  -  Improves Numerical Stability\n",
        "    - Large differences in variable ranges can cause:\n",
        "      - Computational issues\n",
        "      - Poor performance in matrix calculations (especially with many features)\n",
        "\n",
        "  - Essential for Regularization Techniques\n",
        "    - Ridge regression\n",
        "    - Lasso regression\n",
        "    - Elastic Net\n",
        "\n",
        "  - Mitigates Multicollinearity Detection Issues\n",
        "    - Scaling helps in detecting multicollinearity using VIF (Variance Inflation Factor) more clearly.\n",
        "    - Also makes PCA (Principal Component Analysis) more effective, if you're reducing dimensions.\n",
        "\n",
        "  - Improves Convergence in Optimization Algorithms\n",
        "    - For advanced models (e.g., gradient descent–based regression), unscaled features can slow down learning or prevent convergence.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 23. What is polynomial regression ?\n",
        "  - Polynomial regression is an extension of simple linear regression, where we model the relationship between the independent variable (X) and the dependent variable (Y) as an nth-degree polynomial instead of a straight line.\n",
        "\n",
        "  - In simple terms, polynomial regression allows us to fit curved relationships, unlike linear regression which assumes a straight line.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 24. How does polynomial regression differ from linear regression ?\n",
        "  - Key differences:\n",
        "\n",
        "  - 1. Relationship Type:\n",
        "    - Linear Regression:\n",
        "      - Assumes a straight-line relationship between the independent variable(s) and the dependent variable. The model is of the form:\n",
        "    - Polynomial Regression:\n",
        "      - Allows a curved relationship by adding polynomial terms (like X square, X cube, etc)\n",
        "\n",
        "  - 2. Model Complexity:\n",
        "    - Linear Regression:\n",
        "      - Simple, with only one linear term. It’s easy to interpret and visualize, but it may fail to capture more complex patterns.\n",
        "    - Polynomial Regression:\n",
        "      - More complex, especially with higher-degree polynomials. By adding powers of the independent variable, it can fit a wider range of curves, but it becomes harder to interpret as the degree increases.\n",
        "\n",
        "  - 3. Flexibility:\n",
        "    - Linear Regression:\n",
        "      - Limited flexibility. The model can only fit straight-line relationships.\n",
        "    - Polynomial Regression\n",
        "      - Highly flexible. By adding polynomial terms, the model can fit a wide range of curves (parabolas, cubic shapes, etc.), which can be useful for modeling non-linear relationships.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 25. When is polynomial regression used ?\n",
        "  - Polynomial regression is used when the relationship between the independent variable(s) and the dependent variable is non-linear and can't be adequately captured by a straight line (as in simple linear regression). This is where polynomial regression shines\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 26. What is the general equation for polynomial regression ?\n",
        "  - The general equation for polynomial regression is an extension of the linear regression equation but includes higher-degree terms of the independent variable X. It can be written as:\n",
        "\n",
        "  - Y=β\n",
        "0\n",
        "​\n",
        " +β\n",
        "1\n",
        "​\n",
        " X+β\n",
        "2\n",
        "​\n",
        " X\n",
        "2\n",
        " +β\n",
        "3\n",
        "​\n",
        " X\n",
        "3\n",
        " +⋯+β\n",
        "n\n",
        "​\n",
        " X\n",
        "n\n",
        " +ϵ\n",
        "\n",
        " where:\n",
        "\n",
        " - Y = Dependent variable (target you’re trying to predict)\n",
        " - X = Independent variable (predictor)\n",
        " - β\n",
        "0\n",
        "​\n",
        "  = Intercept (constant term)\n",
        " - β\n",
        "1\n",
        "​\n",
        " ,β\n",
        "2\n",
        "​\n",
        " ,…,β\n",
        "n\n",
        "​\n",
        "  = Coefficients for each term (learned by the model)\n",
        "  - X^2, X^3, ... X^n = Polynomial terms of the independent variable\n",
        "𝑋\n",
        "X (squared, cubed, etc.)\n",
        "  - ϵ = Error term (captures the difference between the actual and predicted values)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 27. Can polynomial regression be applied to multiple variables ?\n",
        "  - Yes, polynomial regression can be extended to multiple variables! This is known as Multiple Polynomial Regression. In this case, the independent variables\n",
        "𝑋\n",
        "1\n",
        ",\n",
        "𝑋\n",
        "2\n",
        ",\n",
        "𝑋\n",
        "3\n",
        ",\n",
        "…\n",
        ",\n",
        "𝑋\n",
        "𝑝\n",
        "X\n",
        "1\n",
        "​\n",
        " ,X\n",
        "2\n",
        "​\n",
        " ,X\n",
        "3\n",
        "​\n",
        " ,…,X\n",
        "p\n",
        "​\n",
        "  are raised to different powers and combinations to capture the interactions and non-linear relationships between them and the dependent variable\n",
        "𝑌\n",
        "Y.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 28. What are the limitations of polynomial regression ?\n",
        "  - 1. Overfitting\n",
        "    - As the degree of the polynomial increases, the model becomes more complex and starts to fit the noise in the data rather than the actual pattern.\n",
        "    - Especially risky with small datasets, where higher-degree polynomials can give very high\n",
        "𝑅\n",
        "2\n",
        "R\n",
        "2\n",
        "  values but poor generalization on new data.\n",
        "\n",
        "  - 2. Extrapolation is Dangerous\n",
        "    - Polynomial functions can grow very quickly at the ends (extremes) of the range.\n",
        "    - Predictions outside the range of the data can be wildly inaccurate and unstable.\n",
        "\n",
        "  - 3. Poor Interpretability\n",
        "    - As you add higher-degree and interaction terms, it becomes hard to interpret the influence of individual features.\n",
        "    - For example, understanding what\n",
        "𝑋\n",
        "1\n",
        "3\n",
        "X\n",
        "1\n",
        "3\n",
        "​\n",
        "  or\n",
        "𝑋\n",
        "1\n",
        "𝑋\n",
        "2\n",
        "2\n",
        "X\n",
        "1\n",
        "​\n",
        " X\n",
        "2\n",
        "2\n",
        "​\n",
        "  means in a real-world context isn't always intuitive.\n",
        "\n",
        "  - 4. Sensitive to Outliers\n",
        "    - Polynomial models can be greatly influenced by outliers, especially in higher-degree equations, leading to distorted curves.\n",
        "\n",
        "  - 5. Multicollinearity\n",
        "    - Polynomial terms like\n",
        "𝑋\n",
        "X,\n",
        "𝑋\n",
        "2\n",
        "X\n",
        "2\n",
        " ,\n",
        "𝑋\n",
        "3\n",
        "X\n",
        "3\n",
        "  are often highly correlated with each other.\n",
        "   - This can cause multicollinearity, leading to unstable coefficient estimates and reduced model reliability.\n",
        "  \n",
        "  - 6. Computational Complexity\n",
        "    - As the number of features and polynomial degree increases, the number of terms grows rapidly.\n",
        "    - For\n",
        "𝑝\n",
        "p features and degree\n",
        "𝑑\n",
        "d, the number of terms becomes combinatorially large, making the model slow and memory-heavy.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 29. What methods can be used to evaluate model fit when selecting the degree of a polynomial ?\n",
        "  - 1. Cross-Validation (especially k-fold)\n",
        "    - What it is: Split the dataset into k subsets (folds), train the model on k-1 folds, and test it on the remaining fold — repeat this k times.\n",
        "    - Why use it: Gives a reliable estimate of how well the model generalizes to unseen data.\n",
        "    - How to use it: Try different polynomial degrees and compare their average cross-validation error.\n",
        "\n",
        "  - 2. Mean Squared Error (MSE) / Root Mean Squared Error (RMSE)\n",
        "    - Why use it: Measures how far predictions are from actual values.\n",
        "    - How to use it: Compute MSE on training and validation sets for each degree.\n",
        "      - High Train + High Val Error → Underfitting\n",
        "      - Low Train + High Val Error → Overfitting\n",
        "      - Balanced Low Errors → Good fit\n",
        "\n",
        "  - 3. R² and Adjusted R²\n",
        "    - R²: Proportion of variance in the target explained by the model.\n",
        "    - Adjusted R²: Penalizes for adding irrelevant polynomial terms.\n",
        "    - Why use it: Adjusted R² helps you avoid choosing a high-degree model that doesn’t actually improve performance.\n",
        "\n",
        "  - 4. AIC (Akaike Information Criterion) / BIC (Bayesian Information Criterion)\n",
        "    - Both penalize model complexity:\n",
        "      - AIC = Good for prediction.\n",
        "      - BIC = Stronger penalty for complexity → more conservative.\n",
        "    - Lower AIC/BIC = better model.\n",
        "\n",
        "  - 5. Learning Curves\n",
        "    - Plot training vs validation error as polynomial degree increases.\n",
        "    - Helps visualize bias–variance tradeoff:\n",
        "      - Both errors high = Underfitting\n",
        "      - Train error low, validation high = Overfitting\n",
        "      - Both low and close = Good fit\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 30. Why is visualization important in polynomial regression ?\n",
        "  - 1. Understand the Fit\n",
        "    - Polynomial regression models can create curved or wavy lines, especially at higher degrees.\n",
        "    - A plot shows how well the curve follows the data points.\n",
        "\n",
        "  - 2. Detect Overfitting & Underfitting Visually\n",
        "    - Underfitting → The curve is too simple, doesn’t capture the pattern.\n",
        "    - Overfitting → The curve is too complex, follows noise and outliers.\n",
        "\n",
        "  - 3. Catch Weird Behavior at Edges (Extrapolation)\n",
        "    - Polynomial models can behave unpredictably at the edges of your data range (especially high-degree ones).\n",
        "    - Visualization helps you see if the curve explodes or oscillates outside your data — a major red flag\n",
        "  \n",
        "  - 4. Compare Models Easily\n",
        "    - Plotting multiple polynomial fits (e.g., degree 2 vs degree 5 vs degree 10) lets you compare performance and complexity side by side.\n",
        "  \n",
        "  - 5. Communicate Results\n",
        "    - Graphs are powerful for telling a story to stakeholders, team members, or clients.\n",
        "    - Not everyone understands R² or MSE — but everyone understands a good-looking fit.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 31. How is polynomial regression implemented in Python ?\n",
        "  - 1. Import Libraries\n",
        "  - 2. Create or Load Data\n",
        "  - 3. Transform the Feature\n",
        "  - 4. Fit the Model\n",
        "  - 5. Make Predictions\n",
        "  - 6. Visualize the Results\n",
        "  - 7. Evaluate the Model (Optional)\n",
        "\n",
        "  - so above is the steps to implement the polynomial regression in python.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "\n",
        "  "
      ],
      "metadata": {
        "id": "ME7Y_VgSkyN9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7zCyv0gT6DMh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}